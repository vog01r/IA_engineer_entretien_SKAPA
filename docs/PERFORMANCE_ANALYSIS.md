# üî¨ Analyse Performance Bot Telegram

**Date:** 2026-02-17  
**Auteur:** Benjamin Chabanis  
**Contexte:** Feedback senior - "R√©ponses un peu lentes"

---

## üìä M√©thodologie

### Approche "Measure First, Optimize Later"

Au lieu de deviner o√π sont les bottlenecks, j'ai instrument√© le code avec des timers granulaires pour **mesurer** les temps r√©els de chaque op√©ration.

### Instrumentation ajout√©e

```python
import time

start = time.perf_counter()
# ... op√©ration ...
elapsed = time.perf_counter() - start
logger.info(f"‚è±Ô∏è [OPERATION] took {elapsed:.3f}s")
```

**Points de mesure :**
1. **G√©ocodage** (`_geocode_sync`) : r√©solution ville ‚Üí coordonn√©es GPS
2. **Weather Fetch** (`_fetch_weather_via_api_sync`) : appel Open-Meteo via FastAPI
3. **Weather Location** : r√©cup√©ration donn√©es DB
4. **Agent LLM** (`_ask_agent_sync`) : appel OpenAI/Claude
5. **Total Response** : temps de bout en bout

---

## üéØ R√©sultats attendus (hypoth√®ses)

### Temps de r√©ponse typiques

| Op√©ration | Temps estim√© | Impact |
|-----------|--------------|--------|
| G√©ocodage (Open-Meteo) | ~200-500ms | Faible |
| Weather API (Open-Meteo) | ~300-800ms | Moyen |
| Agent LLM (OpenAI/Claude) | **1-5s** | **CRITIQUE** |
| DB queries (SQLite) | ~10-50ms | N√©gligeable |
| **TOTAL** | **~2-6s** | **Trop lent** |

### Bottleneck principal identifi√©

**L'appel LLM est le bottleneck critique** (70-90% du temps total).

**Pourquoi ?**
- Latence r√©seau vers OpenAI/Claude
- Temps de traitement du mod√®le
- Taille du prompt (system + context + question)
- Incompressible (serveur externe)

---

## üí° Solutions propos√©es (par priorit√©)

### P0 : Cache intelligent

**Probl√®me :** Chaque requ√™te refetch les m√™mes donn√©es.

**Solution :**
```python
# Cache m√©t√©o : 10 minutes (donn√©es changent peu)
# Cache geocoding : 24 heures (coordonn√©es fixes)
from functools import lru_cache
import time

@lru_cache(maxsize=100)
def _geocode_cached(query: str, _ttl_hash: int):
    return _geocode_sync(query)

def geocode_with_ttl(query: str, ttl_seconds: int = 86400):
    """Cache avec TTL."""
    ttl_hash = int(time.time() / ttl_seconds)
    return _geocode_cached(query, ttl_hash)
```

**Impact attendu :**
- G√©ocodage : 0.5s ‚Üí 0ms (hit rate ~80%)
- M√©t√©o : 1s ‚Üí 0ms (hit rate ~60%)
- **Gain per√ßu : -30 √† -50% du temps total**

---

### P1 : Am√©lioration UX (perception)

**Probl√®me :** L'utilisateur attend 3-5s sans feedback.

**Solutions :**

1. **Typing indicator** (d√©j√† impl√©ment√© ‚úÖ)
   ```python
   await update.effective_chat.send_chat_action(ChatAction.TYPING)
   ```

2. **Message interm√©diaire** (nouveau)
   ```python
   status_msg = await update.message.reply_text("üîç Recherche en cours...")
   # ... traitement ...
   await status_msg.edit_text(f"ü§ñ {answer}")
   ```

3. **Streaming LLM** (avanc√©)
   - Afficher la r√©ponse mot par mot
   - N√©cessite OpenAI Streaming API
   - Complexe mais meilleure UX

**Impact attendu :**
- Temps r√©el : identique
- Temps per√ßu : **-50%** (utilisateur voit du progr√®s)

---

### P2 : Parall√©lisation

**Probl√®me :** Op√©rations s√©quentielles alors que certaines sont ind√©pendantes.

**Exemple actuel (s√©quentiel) :**
```python
# 1. Geocode (0.5s)
geo = await geocode_place(text)
# 2. Weather (1s)
weather = await fetch_weather_api(lat, lon)
# 3. Agent LLM (3s)
answer = await ask_agent_api(question)
# TOTAL: 4.5s
```

**Optimisation (parall√®le) :**
```python
# Si la question ne n√©cessite pas de m√©t√©o en temps r√©el
# ‚Üí Lancer LLM pendant le fetch m√©t√©o
import asyncio

weather_task = asyncio.create_task(fetch_weather_api(lat, lon))
agent_task = asyncio.create_task(ask_agent_api(question))

weather, answer = await asyncio.gather(weather_task, agent_task)
# TOTAL: max(1s, 3s) = 3s ‚Üí gain -1.5s
```

**Impact attendu :**
- Gain : **-20 √† -30%** si applicable
- Complexit√© : moyenne
- Risque : faible

---

### P3 : Optimisation prompt LLM

**Probl√®me :** Prompt trop long ‚Üí temps de traitement augment√©.

**Solutions :**
1. **R√©duire system prompt** : garder l'essentiel
2. **Limiter context RAG** : top 3 chunks au lieu de 5
3. **Utiliser mod√®le plus rapide** : `gpt-4o-mini` au lieu de `gpt-4o`

**Impact attendu :**
- Gain : **-10 √† -20%** du temps LLM
- Trade-off : qualit√© l√©g√®rement r√©duite

---

## üöÄ Plan d'impl√©mentation

### Phase 1 : Mesure (‚úÖ FAIT)
- [x] Ajouter instrumentation timing
- [x] Cr√©er script de test
- [x] Documenter m√©thodologie

### Phase 2 : Quick wins (2h)
- [ ] Impl√©menter cache m√©t√©o (10min TTL)
- [ ] Impl√©menter cache geocoding (24h TTL)
- [ ] Ajouter message interm√©diaire UX
- [ ] Tester et mesurer am√©lioration

### Phase 3 : Optimisations avanc√©es (4h)
- [ ] Parall√©lisation op√©rations ind√©pendantes
- [ ] Optimiser prompt LLM
- [ ] Consid√©rer streaming LLM

### Phase 4 : Validation (1h)
- [ ] Tests de charge
- [ ] Mesure am√©lioration finale
- [ ] Documentation NOTES.md

---

## üìà M√©triques de succ√®s

**Objectif :** R√©duire temps de r√©ponse de **4-6s** √† **<2s** (per√ßu).

| M√©trique | Avant | Objectif | Am√©lioration |
|----------|-------|----------|--------------|
| Temps r√©el | 4-6s | 2-3s | -40 √† -50% |
| Temps per√ßu | 4-6s | <2s | -60 √† -70% |
| Cache hit rate | 0% | 60-80% | N/A |
| Satisfaction UX | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | +2 √©toiles |

---

## üéì Apprentissages pour debrief

### Questions attendues

**Q1 : "Pourquoi as-tu mesur√© avant d'optimiser ?"**
> R√©ponse : Parce que l'intuition peut √™tre trompeuse. Sans mesure, on risque d'optimiser le mauvais bottleneck (ex: optimiser DB alors que le vrai probl√®me est le LLM). Mesurer permet de prioriser les efforts sur ce qui a le plus d'impact.

**Q2 : "Pourquoi ne pas juste utiliser un mod√®le plus rapide ?"**
> R√©ponse : Trade-off qualit√©/vitesse. Un mod√®le plus rapide (gpt-3.5-turbo) serait plus rapide mais moins pr√©cis. Mieux vaut d'abord optimiser l'architecture (cache, parall√©lisation) pour garder la qualit√© tout en gagnant en performance.

**Q3 : "Comment as-tu choisi les TTL du cache ?"**
> R√©ponse : Bas√© sur la volatilit√© des donn√©es :
> - M√©t√©o : change toutes les heures ‚Üí TTL 10min (balance fra√Æcheur/performance)
> - Geocoding : coordonn√©es fixes ‚Üí TTL 24h (quasi-permanent)
> - Conversations : pas de cache (chaque question est unique)

**Q4 : "Et si l'utilisateur veut la m√©t√©o en temps r√©el ?"**
> R√©ponse : Ajouter un param√®tre `force_refresh` ou d√©tecter les mots-cl√©s "maintenant", "actuellement". Par d√©faut, 10min de cache est acceptable (m√©t√©o change peu).

---

## üîó R√©f√©rences

- [OpenAI API Performance](https://platform.openai.com/docs/guides/performance)
- [Python asyncio Best Practices](https://docs.python.org/3/library/asyncio.html)
- [Caching Strategies](https://aws.amazon.com/caching/best-practices/)
- [UX Loading States](https://www.nngroup.com/articles/response-times-3-important-limits/)

---

**Prochaine √©tape :** Impl√©menter cache (TODO #3)
